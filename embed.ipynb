{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839a8b6f-9865-4f69-81ae-6feaa6caa3c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Scrape arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264294e5-c6e0-4d54-92c4-86a8a79253a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def scrape_arxiv_day(date_str, subject=\"cs.AI\"):\n",
    "    \"\"\"\n",
    "    Scrapes arXiv 'catchup' listing for a given date and subject.\n",
    "    Returns a pandas DataFrame with arxiv_id, title, authors, abstract, and link columns.\n",
    "    \"\"\"\n",
    "    url = f\"https://arxiv.org/catchup/{subject}/{date_str}?abs=True\"\n",
    "    print(f\"Fetching: {url}\")\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    \n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    articles = []\n",
    "    \n",
    "    # Each paper entry is a <dt> (with link) + <dd> (with metadata)\n",
    "    for dt, dd in zip(soup.find_all(\"dt\"), soup.find_all(\"dd\")):\n",
    "        # Extract arXiv ID and construct full abstract link\n",
    "        abs_tag = dt.find(\"a\", title=\"Abstract\")\n",
    "        if not abs_tag:\n",
    "            continue\n",
    "        arxiv_id = abs_tag.text.strip()\n",
    "        abs_link = \"https://arxiv.org\" + abs_tag[\"href\"]\n",
    "\n",
    "        # Extract title\n",
    "        title_tag = dd.find(\"div\", class_=\"list-title\")\n",
    "        title = title_tag.get_text(strip=True).replace(\"Title:\", \"\").strip() if title_tag else None\n",
    "\n",
    "        # Extract authors\n",
    "        authors_tag = dd.find(\"div\", class_=\"list-authors\")\n",
    "        if authors_tag:\n",
    "            # Each author is an <a> tag inside the div\n",
    "            author_names = [a.get_text(strip=True) for a in authors_tag.find_all(\"a\")]\n",
    "            authors = \", \".join(author_names)\n",
    "        else:\n",
    "            authors = None\n",
    "\n",
    "        # Extract abstract\n",
    "        abs_paragraph = dd.find(\"p\", class_=\"mathjax\")\n",
    "        abstract = abs_paragraph.get_text(strip=True) if abs_paragraph else None\n",
    "\n",
    "        articles.append({\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"abstract\": abstract,\n",
    "            \"link\": abs_link\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    return df\n",
    "\n",
    "def scrape_arxiv_range(start_date:datetime.date, end_date:datetime.date, skip_dates=[], subject=\"cs.AI\"):\n",
    "    \"\"\"\n",
    "    scrapes a date range\n",
    "    \"\"\"\n",
    "    delta = (end_date - start_date).days\n",
    "    dates = [start_date + timedelta(days=i) for i in range(delta + 1)]\n",
    "    dfs = []\n",
    "    for date in dates:\n",
    "        if date not in skip_dates:\n",
    "            d = scrape_arxiv_day(str(date), subject=subject)\n",
    "            d['date'] = str(date)\n",
    "            dfs.append(d)\n",
    "    return pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "def scrape_arxiv_recent(n_days=3, df=None, subject=\"cs.AI\"):\n",
    "    \"\"\"\n",
    "    Just grab the last n days of research data\n",
    "    \"\"\"\n",
    "    start_date = (datetime.now() - timedelta(days=n_days-1)).date()\n",
    "    end_date = datetime.now().date()\n",
    "    skip_dates = [] if df is None or len(df)==0 else [d.date() for d in pd.to_datetime(df['date'].unique())][:-1]\n",
    "    return scrape_arxiv_range(start_date, end_date, skip_dates=skip_dates, subject=subject)\n",
    "\n",
    "def update_paper_data(df_path, n_days=3, subject=\"cs.AI\"):\n",
    "    if os.path.exists(df_path):\n",
    "        df1 = pd.read_pickle(df_path)\n",
    "    else:\n",
    "        df1 = pd.DataFrame()\n",
    "    df2 = scrape_arxiv_recent(n_days=n_days, df=df1, subject=subject)\n",
    "    df = pd.concat([df1, df2]).drop_duplicates(subset='arxiv_id', keep='last')\n",
    "    df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2d2b9e-3c4d-43c3-8fd0-f3b17312844f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-19?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-20?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-21?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-22?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-23?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-24?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-25?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-26?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-27?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-28?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-29?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-09-30?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-01?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-02?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-03?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-04?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-05?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-06?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-07?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-08?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-09?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-10?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-11?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-12?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-13?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-14?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-15?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-16?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-17?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-18?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-19?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-20?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-21?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-22?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-23?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-24?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-25?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-26?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-27?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-28?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-29?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-30?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-10-31?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-01?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-02?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-03?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-04?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-05?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-06?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-07?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-08?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-09?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-10?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-11?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-12?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-13?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-14?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-15?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-16?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-17?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-18?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-19?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-20?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-21?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-22?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-23?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-24?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-25?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-26?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-27?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-28?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-29?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-11-30?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-01?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-02?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-03?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-04?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-05?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-06?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-07?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-08?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-09?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-10?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-11?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-12?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-13?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-14?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-15?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-16?abs=True\n",
      "Fetching: https://arxiv.org/catchup/cs.AI/2025-12-17?abs=True\n"
     ]
    }
   ],
   "source": [
    "update_paper_data('paper_data.pkl', n_days=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cdabd5-7fa8-413f-a203-013d0b75f29f",
   "metadata": {},
   "source": [
    "# Embed the Abstracts and compare against a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0741c2f9-25b6-4297-b1f2-ac67fa3c1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, shutil\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\n",
    "model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n",
    "# model = model.to('cuda')\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_embeddings(model, tokenizer, texts, max_length=8192):\n",
    "    batch_dict = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_dict.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict)\n",
    "        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "        return F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "def embed_df(df_pre):\n",
    "    if os.path.exists('tmp_embed_dfs'):\n",
    "        shutil.rmtree('tmp_embed_dfs')\n",
    "    os.mkdir('tmp_embed_dfs')\n",
    "    for i in tqdm(range(0, len(df_pre), 100)):\n",
    "        df_path = f\"tmp_embed_dfs/{i}.pkl\"\n",
    "        if not os.path.exists(df_path):\n",
    "            df = df_pre.iloc[i:i + 100].reset_index(drop=True).copy(deep=True)\n",
    "            texts = [f\"Title: {t}; Authors: {au}; Abstract: {ab}\" for t, au, ab in df[['title','authors','abstract']].to_numpy()]\n",
    "            \n",
    "            embeddings = get_embeddings(model, tokenizer, texts)\n",
    "                    \n",
    "            df['embedding'] = embeddings.tolist()\n",
    "            df.to_pickle(df_path)\n",
    "            \n",
    "    df_fin = pd.concat([pd.read_pickle(str(p)) for p in Path('tmp_embed_dfs').rglob('*.pkl')]).reset_index(drop=True)\n",
    "    # df_fin.to_pickle('paper_data_embed.pkl')\n",
    "    shutil.rmtree('tmp_embed_dfs')\n",
    "    return df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea43ff0-feee-4da0-acf9-8701d5c11eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcda2b24e6b4d4d8c94a6cebbfd7ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['embedding']) if not os.path.exists('paper_data_embed.pkl') else pd.read_pickle('paper_data_embed.pkl').reset_index(drop=True)\n",
    "df_new = pd.read_pickle('paper_data.pkl').reset_index(drop=True)\n",
    "\n",
    "df_new = pd.concat([df,df_new]).drop_duplicates(subset='arxiv_id', keep='first')\n",
    "df_new = df_new[df_new['embedding'].isna()]\n",
    "\n",
    "if len(df_new)>0:\n",
    "    df_new = embed_df(df_new)\n",
    "    \n",
    "    df = pd.concat([df, df_new]).reset_index(drop=True)\n",
    "    df.to_pickle('paper_data_embed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e1f0eb-473d-4ae2-b630-042141cb384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "texts_of_interest = [\n",
    "    \"Title: AST: Audio Spectrogram Transformer; Authors: Yuan Gong, Yu-An Chung, James Glass; Abstract: In the past decade, convolutional neural networks (CNNs) have been widely adopted as the main building block for end-to-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.\",\n",
    "    \"Title: SSAST: Self-Supervised Audio Spectrogram Transformer; Authors: Yuan Gong, Cheng-I Jeff Lai, Yu-An Chung, James Glass; Abstract: Recently, neural networks based purely on self-attention, such as the Vision Transformer (ViT), have been shown to outperform deep learning models constructed with convolutional neural networks (CNNs) on various vision tasks, thus extending the success of Transformers, which were originally developed for language processing, to the vision domain. A recent study showed that a similar methodology can also be applied to the audio domain. Specifically, the Audio Spectrogram Transformer (AST) achieves state-of-the-art results on various audio classification benchmarks. However, pure Transformer models tend to require more training data compared to CNNs, and the success of the AST relies on supervised pretraining that requires a large amount of labeled data and a complex training pipeline, thus limiting the practical usage of AST. This paper focuses on audio and speech classification, and aims to reduce the need for large amounts of labeled data for AST by leveraging self-supervised learning using unlabeled data. Specifically, we propose to pretrain the AST model with joint discriminative and generative masked spectrogram patch modeling (MSPM) using unlabeled audio from AudioSet and Librispeech. We evaluate our pretrained models on both audio and speech classification tasks including audio event classification, keyword spotting, emotion recognition, and speaker identification. The proposed self-supervised framework significantly boosts AST performance on all tasks, with an average improvement of 60.9%, leading to similar or even better results than a supervised pretrained AST. To the best of our knowledge, it is the first patch-based self-supervised learning framework in the audio and speech domain, and also the first self-supervised learning framework for AST.\",\n",
    "    \"Title: SSAMBA: Self-Supervised Audio Representation Learning with Mamba State Space Model; Authors: Siavash Shams, Sukru Samet Dindar, Xilin Jiang, Nima Mesgarani; Abstract: Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications.\",\n",
    "]\n",
    "\n",
    "weights = torch.tensor([[\n",
    "    1,\n",
    "    0.5,\n",
    "    0.5,\n",
    "]])\n",
    "\n",
    "embeddings_of_interest = get_embeddings(model, tokenizer, texts_of_interest)\n",
    "\n",
    "embeddings = torch.tensor(df['embedding'].tolist())\n",
    "\n",
    "scores = embeddings_of_interest @ embeddings.T\n",
    "weighted_scores = torch.sum(weights.T * scores, dim=0)\n",
    "\n",
    "df = df.iloc[weighted_scores.argsort().flip(dims=[0]).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "379287d0-de9f-4254-88ce-02413d9c598d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In recent years, self-supervised learning has amassed significant interest for training deep neural representations without labeled data. One such self-supervised learning approach is masked spectrogram modeling, where the objective is to learn semantically rich contextual representations by predicting removed or hidden portions of the input audio spectrogram. With the Transformer neural architecture at its core, masked spectrogram modeling has emerged as the prominent approach for learning general purpose audio representations, a.k.a. audio foundation models. Meanwhile, addressing the issues of the Transformer architecture, in particular the underlying Scaled Dot-product Attention operation, which scales quadratically with input sequence length, has led to renewed interest in recurrent sequence modeling approaches. Among them, Selective structured state space models (such as Mamba) and extended Long Short-Term Memory (xLSTM) are the two most promising approaches which have experienced widespread adoption. While the body of work on these two topics continues to grow, there is currently a lack of an adequate overview encompassing the intersection of these topics. In this paper, we present a comprehensive overview of the aforementioned research domains, covering masked spectrogram modeling and the previously mentioned neural sequence modeling architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and xLSTM based masked spectrogram models in a unified, reproducible framework on ten diverse downstream audio classification tasks, which will help interested readers to make informed decisions regarding suitability of the evaluated approaches to adjacent applications.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "date                                                2025-09-24\n",
       "arxiv_id                                      arXiv:2509.18691\n",
       "title        An overview of neural architectures for self-s...\n",
       "authors      Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan\n",
       "abstract     In recent years, self-supervised learning has ...\n",
       "link                          https://arxiv.org/abs/2509.18691\n",
       "embedding    [-0.033794302493333817, -0.025241412222385406,...\n",
       "Name: 8022, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = 0\n",
    "ex = df.iloc[r]\n",
    "print(ex['abstract'])\n",
    "display(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff69b57-1207-4feb-941c-c79668115f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6ce86-423c-4408-9d9e-efc18c4f6dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab353ee-b26b-422f-af5a-e11a4b92ce92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3634aaf-b6bd-40db-b748-43f67ef40e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
